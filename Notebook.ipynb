{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TikTok-to-Spotify Pipeline\n",
    "**TXC Group X**<br>\n",
    "Leticia Brendle - 70033 <br>\n",
    "X<br>\n",
    "X<br>\n",
    "\n",
    "## Research Question\n",
    "\"Does TikTok virality create a 'popularity ceiling' effect? Investigating how TikTok engagement patterns predict and limit long-term streaming success across Spotify.\"\n",
    "\n",
    "\n",
    "\n",
    "### Hypotheses\n",
    "\n",
    "H1: Songs with extremely high TikTok engagement (top 10%) show diminishing returns on Spotify long-term streaming compared to moderate TikTok performers\n",
    "\n",
    "H2: The TikTok-Spotify conversion rate follows an inverted U-shape, with optimal TikTok engagement existing in the middle range\n",
    "\n",
    "Notes:\n",
    "\n",
    "Methods: Polynomial regression to test non-linear relationships + threshold analysis for identifying optimal TikTok engagement levels\n",
    "\n",
    "Why Novel: Tests the counterintuitive idea that TikTok success might actually limit rather than enhance long-term success\n",
    "Business Relevance: Critical for music industry investment and artist development strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import Libraries and Data\n",
    "First we will import all the necessary libraries as well as our data sets. We will also have a first look at the structure of the data to help us understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xfd in position 2679: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstattools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m durbin_watson\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Load CSV file \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m songs_spotify = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msongsspotify.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m## Provide Data Overview ## \u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFirst have a look at the first few rows of our data set and the general structure: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:325\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xfd in position 2679: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "# Data\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Graphs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistics\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Load CSV file \n",
    "songs_spotify = pd.read_csv('songsspotify.csv', encoding='cp1252')\n",
    "\n",
    "## Provide Data Overview ## \n",
    "print(\"First have a look at the first few rows of our data set and the general structure: \\n\")\n",
    "print(songs_spotify.head())\n",
    "print(\"\\n\\nNumber of Rows and Columns: \" + str(songs_spotify.shape) + \"\\n\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data Cleaning\n",
    "\n",
    "clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 4600 rows, 29 columns\n",
      "\n",
      "3.3.1 Drop Unnecessary Columns\n",
      "----------------------------------------\n",
      "Keeping 11 relevant columns from 29 total columns:\n",
      "  • Track\n",
      "  • Artist\n",
      "  • Release Date\n",
      "  • Spotify Streams\n",
      "  • Spotify Popularity\n",
      "  • Spotify Playlist Count\n",
      "  • TikTok Posts\n",
      "  • TikTok Likes\n",
      "  • TikTok Views\n",
      "  • Track Score\n",
      "  • All Time Rank\n",
      "\n",
      "Dataset after dropping columns: 4600 rows, 11 columns\n",
      "\n",
      "3.3.2 Missing Values\n",
      "----------------------------------------\n",
      "Missing values per column:\n",
      "  Track: 0 (0.0%)\n",
      "  Artist: 5 (0.1%)\n",
      "  Release Date: 0 (0.0%)\n",
      "  Spotify Streams: 113 (2.5%)\n",
      "  Spotify Popularity: 804 (17.5%)\n",
      "  Spotify Playlist Count: 70 (1.5%)\n",
      "  TikTok Posts: 1173 (25.5%)\n",
      "  TikTok Likes: 980 (21.3%)\n",
      "  TikTok Views: 981 (21.3%)\n",
      "  Track Score: 0 (0.0%)\n",
      "  All Time Rank: 0 (0.0%)\n",
      "\n",
      "Rows before dropping missing values: 4600\n",
      "Rows after dropping missing values: 3171\n",
      "\n",
      "Double-checking for missing values after dropping:\n",
      "✅ No missing values remaining!\n",
      "\n",
      "3.3.3 Duplicates\n",
      "----------------------------------------\n",
      "Checking for duplicates based on Track + Artist combination...\n",
      "Duplicate songs found: 12\n",
      "\n",
      "Examples of duplicate songs:\n",
      "  'Bad and Boujee (feat. Lil Uzi Vert)' by Migos: 2 entries\n",
      "  'Cheap Thrills' by Sia: 2 entries\n",
      "  'Dembow' by Danny Ocean: 2 entries\n",
      "  'Let Her Go' by Passenger: 2 entries\n",
      "  'Me Rehï¿½ï' by Danny Ocean: 2 entries\n",
      "\n",
      "Duplicates removed: 12\n",
      "Double-checking for duplicates after removal: 0\n",
      "Final dataset: 3159 rows, 11 columns\n",
      "\n",
      "==================================================\n",
      "DATA CLEANING SUMMARY\n",
      "==================================================\n",
      "Original dataset: 4600 rows, 29 columns\n",
      "After dropping columns: 11 columns kept\n",
      "After handling missing values: 3159 rows remaining\n",
      "After removing duplicates: 3159 unique songs\n",
      "Final missing values: 0\n",
      "\n",
      "Dataset ready for TikTok-to-Spotify Pipeline analysis!\n",
      "Key variables available:\n",
      "  • TikTok Metrics: Posts, Likes, Views\n",
      "  • Spotify Metrics: Streams, Popularity, Playlist Count\n",
      "  • Control Variables: Track Score, All Time Rank, Release Date\n",
      "\n",
      "Songs with both TikTok and Spotify data: 3159 (100.0%)\n",
      "\n",
      "Cleaned dataset saved as 'spotify_cleaned.csv'\n",
      "Ready for Exploratory Data Analysis! 🚀\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataset laden\n",
    "df = pd.read_csv('songsspotify.csv', encoding='cp1252')\n",
    "print(f\"Original dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3.3 DATA CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "# 3.3.1 DROP UNNECESSARY COLUMNS\n",
    "print(\"\\n3.3.1 Drop Unnecessary Columns\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Für TikTok-to-Spotify Pipeline relevante Spalten\n",
    "relevant_columns = [\n",
    "    'Track', 'Artist', 'Release Date',\n",
    "    'Spotify Streams', 'Spotify Popularity', 'Spotify Playlist Count', \n",
    "    'TikTok Posts', 'TikTok Likes', 'TikTok Views',\n",
    "    'Track Score', 'All Time Rank'\n",
    "]\n",
    "\n",
    "print(f\"Keeping {len(relevant_columns)} relevant columns from {df.shape[1]} total columns:\")\n",
    "for col in relevant_columns:\n",
    "    print(f\"  • {col}\")\n",
    "\n",
    "# Unnötige Spalten droppen\n",
    "df_clean = df[relevant_columns].copy()\n",
    "print(f\"\\nDataset after dropping columns: {df_clean.shape[0]} rows, {df_clean.shape[1]} columns\")\n",
    "\n",
    "# 3.3.2 MISSING VALUES\n",
    "print(\"\\n3.3.2 Missing Values\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Missing values analysieren\n",
    "missing_summary = df_clean.isnull().sum()\n",
    "missing_percentage = (missing_summary / len(df_clean)) * 100\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "for col in df_clean.columns:\n",
    "    missing_count = missing_summary[col]\n",
    "    missing_pct = missing_percentage[col]\n",
    "    print(f\"  {col}: {missing_count} ({missing_pct:.1f}%)\")\n",
    "\n",
    "# Numerische Spalten identifizieren und Kommas entfernen\n",
    "numeric_columns = [\n",
    "    'Spotify Streams', 'Spotify Popularity', 'Spotify Playlist Count',\n",
    "    'TikTok Posts', 'TikTok Likes', 'TikTok Views', \n",
    "    'Track Score', 'All Time Rank'\n",
    "]\n",
    "\n",
    "# String-Zahlen zu numerisch konvertieren (Kommas entfernen)\n",
    "for col in numeric_columns:\n",
    "    if col in df_clean.columns:\n",
    "        # Kommas entfernen und zu numerisch konvertieren\n",
    "        df_clean[col] = df_clean[col].astype(str).str.replace(',', '').str.replace(' ', '')\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Release Date zu datetime\n",
    "df_clean['Release Date'] = pd.to_datetime(df_clean['Release Date'], errors='coerce')\n",
    "\n",
    "# Rows mit missing values droppen\n",
    "print(f\"\\nRows before dropping missing values: {len(df_clean)}\")\n",
    "df_clean = df_clean.dropna()\n",
    "print(f\"Rows after dropping missing values: {len(df_clean)}\")\n",
    "\n",
    "# Nochmal nach missing values checken\n",
    "print(f\"\\nDouble-checking for missing values after dropping:\")\n",
    "remaining_missing = df_clean.isnull().sum()\n",
    "total_missing = remaining_missing.sum()\n",
    "\n",
    "for col in df_clean.columns:\n",
    "    if remaining_missing[col] > 0:\n",
    "        print(f\"  {col}: {remaining_missing[col]} missing values\")\n",
    "    \n",
    "if total_missing == 0:\n",
    "    print(\"✅ No missing values remaining!\")\n",
    "else:\n",
    "    print(f\"⚠️ Still {total_missing} missing values found!\")\n",
    "    print(\"Dropping remaining rows with any missing values...\")\n",
    "    df_clean = df_clean.dropna()\n",
    "    print(f\"Final rows after complete cleaning: {len(df_clean)}\")\n",
    "    \n",
    "    # Final check\n",
    "    final_missing = df_clean.isnull().sum().sum()\n",
    "    if final_missing == 0:\n",
    "        print(\"✅ All missing values successfully removed!\")\n",
    "    else:\n",
    "        print(f\"❌ Still {final_missing} missing values remaining\")\n",
    "\n",
    "# 3.3.3 DUPLICATES\n",
    "print(\"\\n3.3.3 Duplicates\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Duplicates basierend auf Track + Artist identifizieren\n",
    "print(\"Checking for duplicates based on Track + Artist combination...\")\n",
    "\n",
    "duplicates_before = df_clean.duplicated(subset=['Track', 'Artist']).sum()\n",
    "print(f\"Duplicate songs found: {duplicates_before}\")\n",
    "\n",
    "if duplicates_before > 0:\n",
    "    print(\"\\nExamples of duplicate songs:\")\n",
    "    duplicate_songs = df_clean[df_clean.duplicated(subset=['Track', 'Artist'], keep=False)]\n",
    "    duplicate_examples = duplicate_songs.groupby(['Track', 'Artist']).size().head()\n",
    "    for (track, artist), count in duplicate_examples.items():\n",
    "        print(f\"  '{track}' by {artist}: {count} entries\")\n",
    "    \n",
    "    # Duplicates droppen (erste Occurrence behalten)\n",
    "    rows_before_dup_removal = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates(subset=['Track', 'Artist'], keep='first')\n",
    "    duplicates_removed = rows_before_dup_removal - len(df_clean)\n",
    "    print(f\"\\nDuplicates removed: {duplicates_removed}\")\n",
    "else:\n",
    "    print(\"✅ No duplicates found!\")\n",
    "\n",
    "# Nochmal nach duplicates checken\n",
    "duplicates_after = df_clean.duplicated(subset=['Track', 'Artist']).sum()\n",
    "print(f\"Double-checking for duplicates after removal: {duplicates_after}\")\n",
    "print(f\"Final dataset: {df_clean.shape[0]} rows, {df_clean.shape[1]} columns\")\n",
    "\n",
    "# =============================================================================\n",
    "# CLEANING SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA CLEANING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"After dropping columns: {len(relevant_columns)} columns kept\")\n",
    "print(f\"After handling missing values: {len(df_clean)} rows remaining\")\n",
    "print(f\"After removing duplicates: {len(df_clean)} unique songs\")\n",
    "print(f\"Final missing values: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\nDataset ready for TikTok-to-Spotify Pipeline analysis!\")\n",
    "print(f\"Key variables available:\")\n",
    "print(f\"  • TikTok Metrics: Posts, Likes, Views\")\n",
    "print(f\"  • Spotify Metrics: Streams, Popularity, Playlist Count\")\n",
    "print(f\"  • Control Variables: Track Score, All Time Rank, Release Date\")\n",
    "\n",
    "# Quick data quality check\n",
    "songs_with_both_data = ((df_clean['TikTok Posts'] > 0) | (df_clean['TikTok Likes'] > 0)) & (df_clean['Spotify Streams'] > 0)\n",
    "print(f\"\\nSongs with both TikTok and Spotify data: {songs_with_both_data.sum()} ({songs_with_both_data.mean():.1%})\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_clean.to_csv('spotify_cleaned.csv', index=False)\n",
    "print(f\"\\nCleaned dataset saved as 'spotify_cleaned.csv'\")\n",
    "print(\"Ready for Exploratory Data Analysis! 🚀\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
